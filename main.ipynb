{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "interpreter": {
      "hash": "2d04b9be2429acaaca541b9fd5f04e1d13e142a6d8a3b01163614a22cbe6d5d6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# !git clone 'https://github.com/Lorenz92/SKADC1.git'\r\n",
        "# % cd SKADC1\r\n",
        "# !echo $PWD"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import src.dataset as dataset\r\n",
        "import src.config as config \r\n",
        "from src.utils import *\r\n",
        "import src.models as models\r\n",
        "import src.losses as loss\r\n",
        "\r\n",
        "path = config.TRAIN_PATCHES_FOLDER\r\n",
        "%load_ext autoreload\r\n",
        "%autoreload 2\r\n",
        "\r\n",
        "np.random.seed(config.RANDOM_SEED)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7WwOlsifF-5G",
        "outputId": "303002ab-c8c4-4807-caeb-ee1c590d4326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if 'google.colab' in str(get_ipython()):\r\n",
        "  use_colab = True\r\n",
        "  print('Running on CoLab')\r\n",
        "else:\r\n",
        "  use_colab = False\r\n",
        "  print('Not running on CoLab')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if use_colab:\r\n",
        "    # Read file from Colab Notebook\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount('/content/drive')\r\n",
        "    config.MODEL_WEIGHTS = \"/content/drive/My Drive/Colab Notebooks/SKADC1\"\r\n",
        "    config.IMAGE_PATH = \"/content/drive/My Drive/Colab Notebooks/SKADC1/asset/560Mhz_1000h.fits\""
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Dataset parsing and loading\r\n",
        "# use \"subset\" in config file to load a small portion of data for development/debugging purposes\r\n",
        "ska_dataset = dataset.SKADataset(k=3, print_info=False, use_pb=False)"
      ],
      "outputs": [],
      "metadata": {
        "tags": [],
        "id": "Q8IbYCwkF-5K",
        "outputId": "5cd8e5ba-49cd-49b4-d65c-65d82229f792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "ska_dataset.cleaned_train_df[['width', 'height', 'area_orig', 'area_cropped']].describe()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "ska_dataset.cleaned_train_df[['width', 'height', 'area_orig']].quantile([.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,.98,.99,1.])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "show_plot = True\r\n",
        "RGB_patch_norm = True\r\n",
        "ska_dataset.generate_patches(limit=2, patch_RGB_norm =  RGB_patch_norm, plot_patches=show_plot) # Remember to remove internal return"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ug10Ku-OF-5Y",
        "outputId": "c3b6bed6-24a7-4f71-cbed-c4b6b8d934af",
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "ska_dataset.analyze_class_distribution()"
      ],
      "outputs": [],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "ska_dataset.split_train_val_stratified(random_state=42, val_portion=0.2)\r\n",
        "\r\n",
        "print(len(ska_dataset.train_patch_list))\r\n",
        "print(len(ska_dataset.val_patch_list))\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "ska_dataset.split_train_val(random_state=42, val_portion=0.2)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## datagen + calc_rpn -> rpn_net -> rpn_to_roi -> calc_iou -> cls_net"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print_img(config.TRAIN_PATCHES_FOLDER, '20_16396_16729_20', show_data=True)\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Debugging\r\n",
        "\r\n",
        "train_patch_list = ska_dataset.train_patch_list\r\n",
        "patches_folder_path=config.TRAIN_PATCHES_FOLDER\r\n",
        "\r\n",
        "train_datagen = prep.get_anchor_gt(patches_folder_path, ['20_16396_16729_20'], backbone, pixel_mean=None)\r\n",
        "image, [y_rpn_cls_true, y_rpn_reg_true], img_data_aug, _, _, patch_id = next(train_datagen)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get FRCNN model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Choose the feature extraction model\r\n",
        "backbone='baseline_16'\r\n",
        "# backbone='vgg16'\r\n",
        "\r\n",
        "if backbone=='baseline_16':\r\n",
        "    config.resizePatch = True\r\n",
        "    config.rpn_stride = 4\r\n",
        "    config.num_rois = 16\r\n",
        "    # config.anchor_box_scales = [16,32,64] # anchors in the original image size\r\n",
        "    config.anchor_box_scales = [6, 8, 12, 16, 24, 32]\r\n",
        "    config.resizeFinalDim = 100\r\n",
        "    input_shape_1 = config.resizeFinalDim\r\n",
        "else:\r\n",
        "    config.resizePatch = True\r\n",
        "    config.rpn_stride = 16\r\n",
        "    config.num_rois = 4\r\n",
        "    config.resizeFinalDim = 600\r\n",
        "    input_shape_1=config.resizeFinalDim\r\n",
        "    config.anchor_box_scales = [32, 64, 128]\r\n",
        "\r\n",
        "\r\n",
        "config.anchor_num = len(config.anchor_box_ratios)*len(config.anchor_box_scales)\r\n",
        "input_shape_2=(None, 4)\r\n",
        "\r\n",
        "print(config.resizePatch)\r\n",
        "print(config.rpn_stride)\r\n",
        "\r\n",
        "checkpoint = get_model_last_checkpoint(backbone)\r\n",
        "print(f'Model last checkpoint: {checkpoint}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rpn_model, detector_model, total_model = models.get_train_model(input_shape_1=input_shape_1, input_shape_2=input_shape_2, anchor_num=config.anchor_num, pooling_regions=config.pooling_regions, num_rois=config.num_rois, num_classes=len(ska_dataset.class_list)+1, backbone=backbone, use_expander=False)\r\n",
        "\r\n",
        "rpn_model.summary()\r\n",
        "detector_model.summary()\r\n",
        "total_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load weights"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "models.load_weigths(rpn_model, detector_model, backbone, resume_train=False, checkpoint=checkpoint)\r\n",
        "models.compile_models(rpn_model, detector_model, total_model, rpn_losses=[loss.rpn_loss_cls, loss.rpn_loss_regr], detector_losses=[loss.detector_loss_cls, loss.detector_loss_regr], class_list=ska_dataset.class_list)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Specifically checking backbone weights\r\n",
        "\r\n",
        "# total_model.weights[24:25][0][0][0][0]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Check that all of the pretrained weights have been loaded.\r\n",
        "import numpy as np\r\n",
        "for i, j in zip(total_model.weights, rpn_model.weights): \r\n",
        "    # print(i,j)\r\n",
        "    assert np.allclose(i,j), 'Weights don\\'t match!'"
      ],
      "outputs": [],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- errore \"Exception: 'a' cannot be empty unless no samples are taken\" quando nel sampling ci sono meno di 4 roi\n",
        "- errore \"None type object is not iterable\" dovuto al max(IoUs) quando calc_iou torna None, None, None, None\n",
        "- patch di debug 1550_16376_16779_100\n",
        "- capire il parametro classifier_regr_std in che modo influenza il training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "- provare normalizzazione valori immagini di input\n",
        "- provare a far passare più roi anzichè 4"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# zzz = np.load(f'{config.TRAIN_PATCHES_FOLDER}/3100_16376_16829_100/3100_16376_16829_100.npy')\r\n",
        "\r\n",
        "# print_img(config.TRAIN_PATCHES_FOLDER, '3100_16376_16829_100')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "20_16396_16729_20"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from src.train import *\r\n",
        "val_patch_list = None\r\n",
        "train_frcnn(rpn_model, detector_model, total_model, ['20_16396_16729_20'], ska_dataset.val_patch_list, ska_dataset.class_list, num_epochs=500, patches_folder_path=config.TRAIN_PATCHES_FOLDER, single_patch_norm = RGB_patch_norm, backbone=backbone, resume_train=True)\r\n",
        "# train_frcnn(rpn_model, detector_model, total_model, ska_dataset.train_patch_list, ska_dataset.val_patch_list, ska_dataset.class_list, num_epochs=300, patches_folder_path=config.TRAIN_PATCHES_FOLDER, backbone=backbone, resume_train=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "total_model.save_weights(f'{config.MODEL_WEIGHTS}/{backbone}/0_frcnn_{backbone}.h5')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rpn_model_eval, detector_model_eval, total_model_eval = models.get_eval_model(input_shape_1=input_shape_1, input_shape_2=input_shape_2, input_shape_fmap=None, anchor_num=config.anchor_num, pooling_regions=config.pooling_regions, num_rois=config.num_rois, num_classes=len(ska_dataset.class_list)+1, backbone=backbone, use_expander=False)\r\n",
        "\r\n",
        "rpn_model_eval.summary()\r\n",
        "detector_model_eval.summary()\r\n",
        "total_model_eval.summary()"
      ],
      "outputs": [],
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Models used for mAP eval\r\n",
        "cp = '262_frcnn_baseline_16.h5'\r\n",
        "models.load_weigths(rpn_model_eval, detector_model_eval, backbone, checkpoint=cp)\r\n",
        "models.compile_models(rpn_model_eval, detector_model_eval, total_model_eval, rpn_losses=[loss.rpn_loss_cls, loss.rpn_loss_regr], detector_losses=[loss.detector_loss_cls, loss.detector_loss_regr], class_list=ska_dataset.class_list)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "patch_id = ['20_16396_16729_20']#ska_dataset.train_patch_list#[1:2]\r\n",
        "# print(patch_id)\r\n",
        "# gt = pd.read_pickle(f'{config.TRAIN_PATCHES_FOLDER}/{patch_id[0]}/{patch_id[0]}.pkl')\r\n",
        "# display(gt['class_label'])\r\n",
        "\r\n",
        "preds, mAP, prec, recall = evaluate_model(rpn_model_eval, detector_model_eval, backbone, patch_id, ska_dataset.class_list, metric_threshold=.5)"
      ],
      "outputs": [],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "preds"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(config.patch_dim , float(config.resizeFinalDim))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print_img(config.TRAIN_PATCHES_FOLDER, '20_16396_16729_20')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anch = pd.read_pickle(f'{config.EVAL_RESULTS}/20_16396_16729_20/20_16396_16729_20.pkl')\r\n",
        "display(anch)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anch['width'] = anch['x2s'] - anch['x1s']\r\n",
        "anch['heght'] = anch['y2s'] - anch['y1s']"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anch.describe() "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print_img(config.TRAIN_PATCHES_FOLDER, '20_16396_16729_20', config.EVAL_RESULTS)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - 20210508:\n",
        "- [X] troncamento rumore a 1e-6 tramite half gaussian\n",
        "\n",
        "TODO - 20210605:\n",
        "- [X] modificare RPNNet in modo che dia in output anche la backbone - Lorenzo\n",
        "- [X] scrivere bene training loop: salvare le loss in un df su disco + salvare pesi modello ad ogni giro (Lorenzo - finire di debuggare)\n",
        "\n",
        "TODO - 20210620:\n",
        "- [x] implementare mAP in una funzione che prende come parametro un modello o i suoi pesi\n",
        "- [x] implementare resNet50\n",
        "- [x] implementare predicted rois - Lorenzo\n",
        "- [X] implementare plot loss training - Lorenzo\n",
        "- [X] finire classe datasetv2 - Alice\n",
        "- [X] check se su colab le performance sono migliori - Lorenzo\n",
        "\n",
        "TODO - 20210627\n",
        "- [X] split dataset su combinazioni classi - Alice\n",
        "- [x] provare campionamento random patch ed osservare le due distribuzioni - Alice\n",
        "\n",
        "TODO - 20210703\n",
        "- [x] sistemare salvataggio loss training loop - Lorenzo\n",
        "- [x] Riscalare immagini tra 0-255 - Alice\n",
        "- [x] capire se passare tre immagini diverse come input\n",
        "- [x] usare media vgg16 per zero-centering - Alice\n",
        "\n",
        "TODO - 20210705\n",
        "- [x] sistemare nomi funzioni dataset per trasformazione rgb\n",
        "\n",
        "TODO - 20210711\n",
        "- [x] rifattorizzare classe dataset spostando nel costruttore i metodi che calcolano i suoi attributi - Lorenzo\n",
        "- [x] chek valori pixel in input per resnet\n",
        "- [x] fare funzione per plottare le predictions\n",
        "- [ ] trainare tutto su colab\n",
        "\n",
        "TODO - 20210714\n",
        "- [x] ragionare su come scalare le immagini fra 0 e 1, attualmente hanno tanti valori schiacciati a 0 e il massimo su tutto il train a a 0.4\n",
        "\n",
        "TODO - 20210717\n",
        "- [ ] Ablation study: provare a rimuovere stage4 nella resnet - se c'è tempo\n",
        "- [x] Provare con nostra pixel_mean e con vgg16 pixel_mean -> per il momento abbiamo scartato la prima opzione\n",
        "- [ ] Fare qualche analisi di distribuzione delle classi/dim box del dataset - Alice\n",
        "- [x] Aggiungere normalizzazione dopo zero centering per resNet50, sulla base del range globale dell'immagine di training\n",
        "- [ ] Provare pulizia dataset originale sulla base del rumore/flusso - Alice\n",
        "- [ ] Cambiare nomi di tutto - alla fine\n",
        "- [x] implementare zero-centering su volare medio RGB delle nostre patch\n",
        "- [x] Funzione che trova l'ultimo checkpoint in colab prima del load_weights - Lorenzo\n",
        "\n",
        "TODO - 20210801\n",
        "- [x] Debuggare training baseline 8 e 16 - L\n",
        "- [ ] Finire prove pulizia dataseet noise variando k - A"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Summary\n",
        "    - riassunto progetto\n",
        "2.  Background\n",
        "    - SoTA + teoria di base\n",
        "3.  System Description\n",
        "    - descrizione dei nostri modelli e dei loro componenti (moduli)\n",
        "4.  Experimental setup and results\n",
        "    - dataset pre processing\n",
        "    - training environment\n",
        "    - metrics\n",
        "    - results\n",
        "5.  Results and error analysis\n",
        "6.  Discussion"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "loss_history = np.load(f\"./model/{backbone}/loss_history.npy\")\n",
        "print(loss_history.shape)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "loss_history"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plot_loss(loss_history)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ]
}