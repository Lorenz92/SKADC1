@misc{bert,
  abstract  = {We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations by jointly conditioning on both left and right
context in all layers. As a result, the pre-trained BERT representations can be
fine-tuned with just one additional output layer to create state-of-the-art
models for a wide range of tasks, such as question answering and language
inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI
accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question
answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human
performance by 2.0%.},
  added-at  = {2019-02-05T23:35:51.000+0100},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords  = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  note      = {cite arxiv:1810.04805Comment: 13 pages},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url       = {http://arxiv.org/abs/1810.04805},
  year      = 2018
}

@online{bidaf-medium,
  author = {Meraldo Antonio},
  title  = {An Illustrated Guide to Bi-Directional Attention Flow (BiDAF)},
  url    = {https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b}
}

@article{bidaf,
  author        = {Min Joon Seo and
               Aniruddha Kembhavi and
               Ali Farhadi and
               Hannaneh Hajishirzi},
  title         = {Bidirectional Attention Flow for Machine Comprehension},
  journal       = {CoRR},
  volume        = {abs/1611.01603},
  year          = {2016},
  url           = {http://arxiv.org/abs/1611.01603},
  archiveprefix = {arXiv},
  eprint        = {1611.01603},
  timestamp     = {Mon, 13 Aug 2018 16:46:34 +0200}
}

@article{glue,
  author        = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title         = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal       = {CoRR},
  volume        = {abs/1804.07461},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.07461},
  archiveprefix = {arXiv},
  eprint        = {1804.07461},
  timestamp     = {Mon, 13 Aug 2018 16:46:56 +0200}
}

@online{colab,
  author = {Google},
  title  = {Colaboratory},
  url    = {https://colab.research.google.com/}
}

@article{distilbert,
  author        = {Victor Sanh and
               Lysandre Debut and
               Julien Chaumond and
               Thomas Wolf},
  title         = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
               and lighter},
  journal       = {CoRR},
  volume        = {abs/1910.01108},
  year          = {2019},
  url           = {http://arxiv.org/abs/1910.01108},
  archiveprefix = {arXiv},
  eprint        = {1910.01108},
  timestamp     = {Tue, 02 Jun 2020 12:48:59 +0200}
}

@online{distilbert-article,
  author = {Victor Sanh},
  title  = {Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT},
  url    = {https://medium.com/huggingface/distilbert-8cf3380435b5}
}

@misc{distillation,
  title         = {Distilling the Knowledge in a Neural Network},
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  year          = {2015},
  eprint        = {1503.02531},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@misc{electra,
  title         = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author        = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  year          = {2020},
  eprint        = {2003.10555},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{glove,
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/D14-1162},
  doi       = {10.3115/v1/D14-1162},
  pages     = {1532--1543}
}

@inproceedings{gru,
  title     = {Deep Gate Recurrent Neural Network},
  author    = {Yuan Gao and Dorota Glowacka},
  booktitle = {Proceedings of The 8th Asian Conference on Machine Learning},
  pages     = {350--365},
  year      = {2016},
  editor    = {Robert J. Durrant and Kee-Eung Kim},
  volume    = {63},
  series    = {Proceedings of Machine Learning Research},
  address   = {The University of Waikato, Hamilton, New Zealand},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v63/gao30.pdf},
  url       = {http://proceedings.mlr.press/v63/gao30.html},
  abstract  = {This paper explores the possibility of using multiplicative gates to build two recurrent neural network structures. These two structures are called Deep Simple Gated Unit (DSGU) and Simple Gated Unit (SGU), which are structures for learning long-term dependencies. Compared to traditional Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), both structures require fewer parameters and less computation time in sequence classification tasks. Unlike GRU and LSTM, which require more than one gate to control information flow in the network, SGU and DSGU only use one multiplicative gate to control the flow of information. We show that this difference can accelerate the learning speed in tasks that require long dependency information. We also show that DSGU is more numerically stable than SGU. In addition, we also propose a standard way of representing the inner structure of RNN called RNN Conventional Graph (RCG), which helps to analyze the relationship between input units and hidden units of RNN.}
}

@article{highway-network,
  author        = {Rupesh Kumar Srivastava and
               Klaus Greff and
               J{\"{u}}rgen Schmidhuber},
  title         = {Highway Networks},
  journal       = {CoRR},
  volume        = {abs/1505.00387},
  year          = {2015},
  url           = {http://arxiv.org/abs/1505.00387},
  archiveprefix = {arXiv},
  eprint        = {1505.00387},
  timestamp     = {Mon, 13 Aug 2018 16:48:21 +0200}
}

@article{huggingface,
  author        = {Thomas Wolf and
               Lysandre Debut and
               Victor Sanh and
               Julien Chaumond and
               Clement Delangue and
               Anthony Moi and
               Pierric Cistac and
               Tim Rault and
               R{\'{e}}mi Louf and
               Morgan Funtowicz and
               Jamie Brew},
  title         = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal       = {CoRR},
  volume        = {abs/1910.03771},
  year          = {2019},
  url           = {http://arxiv.org/abs/1910.03771},
  archiveprefix = {arXiv},
  eprint        = {1910.03771},
  timestamp     = {Tue, 02 Jun 2020 12:49:01 +0200}
}


@article{lstm,
  author   = {Sepp Hochreiter and JÃ¼rgen Schmidhuber},
  journal  = {Neural Computation},
  title    = {Long Short-Term Memory},
  year     = {1997},
  number   = {8},
  pages    = {1735--1780},
  volume   = {9},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  doi      = {10.1162/neco.1997.9.8.1735}
}

@techreport{max-context-tokens,
  title       = {{Question Answering on the SQuAD Dataset}},
  author      = {Do-Hyoung Park and Vihan Lakshman},
  institution = {Stanford University},
  month       = {01}
}

@inproceedings{ppmi,
  author    = {Niwa, Yoshiki and Nitta, Yoshihiko},
  title     = {Co-Occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries},
  year      = {1994},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  url       = {https://doi.org/10.3115/991886.991938},
  doi       = {10.3115/991886.991938},
  abstract  = {A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors.},
  booktitle = {Proceedings of the 15th Conference on Computational Linguistics - Volume 1},
  pages     = {304â309},
  numpages  = {6},
  series    = {COLING '94}
}

@incollection{pytorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@online{recurrent-img,
  author = {Nikolai Morin},
  title  = {What's the difference between âhiddenâ and âoutputâ in PyTorch LSTM?},
  url    = {https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm}
}

@online{ska-site,
  author = {SKA},
  title  = {SKA web site},
  url    = {https://www.skatelescope.org/news/ska-launches-science-data-challenge/}
}

@inbook{tf-idf,
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  title     = {TF--IDF},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {986--987},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_832},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_832}
}

@article{transformers,
  author        = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title         = {Attention Is All You Need},
  journal       = {CoRR},
  volume        = {abs/1706.03762},
  year          = {2017},
  url           = {http://arxiv.org/abs/1706.03762},
  archiveprefix = {arXiv},
  eprint        = {1706.03762}
}

@misc{wandb,
  title  = {Experiment Tracking with Weights and Biases},
  year   = {2020},
  note   = {Software available from the site wandb.com},
  url    = {https://www.wandb.com/},
  author = {Biewald, Lukas}
}

@incollection{word2vec,
  added-at  = {2015-08-28T18:25:59.000+0200},
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {NIPS},
  interhash = {4d7ff49f008ec05928f11e50f2db1cf9},
  intrahash = {4068b331120ae60adb367eb4cd1b7407},
  keywords  = {blog_semnav_wiki deep deepwiki learning proposal solvatio tau word2vec},
  pages     = {3111--3119},
  publisher = {Curran Associates, Inc.},
  timestamp = {2018-02-28T16:31:16.000+0100},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  url       = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
  year      = 2013
}

@article{wordpiece,
  author        = {Yonghui Wu,
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title         = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal       = {CoRR},
  volume        = {abs/1609.08144},
  year          = {2016},
  url           = {http://arxiv.org/abs/1609.08144},
  archiveprefix = {arXiv},
  eprint        = {1609.08144},
  timestamp     = {Thu, 14 Jan 2021 12:12:19 +0100}
}

@online{tokenizers,
  author = {HuggingFace},
  title  = {HuggingFace tokenizers},
  url    = {https://huggingface.co/docs/tokenizers/python/latest/}
}

@online{unk,
  author = {Pennington, Jeffrey},
  title  = {Unknown word tag},
  url    = {https://groups.google.com/g/globalvectors/c/9w8ZADXJclA/m/hRdn4prm-XUJ?pli=1}
}
